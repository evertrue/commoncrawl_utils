// File generated by rpc compiler. Do not edit.

package org.commoncrawl.protocol;

import java.io.DataInput;
import java.io.DataOutput;
import java.util.BitSet;
import java.io.IOException;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.record.Buffer;
import org.commoncrawl.util.shared.FlexBuffer;
import org.commoncrawl.util.shared.TextBytes;
import org.commoncrawl.util.shared.MurmurHash;
import org.commoncrawl.util.shared.ImmutableBuffer;
import org.commoncrawl.rpc.BinaryProtocol;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.conf.Configuration;

// Generated File: ArchiveInfo
public class ArchiveInfo extends org.commoncrawl.rpc.RPCStruct<ArchiveInfo> implements Writable {

	// optimized constructor helper 
	public static ArchiveInfo newInstance(Configuration conf) {
		return ReflectionUtils.newInstance(ArchiveInfo.class, conf);
	}

	// Writable Implementation
	public void write(DataOutput out) throws IOException {
		this.serialize(out, new BinaryProtocol());
	}

	public void readFields(DataInput in) throws IOException {
		this.deserialize(in, new BinaryProtocol());
	}

	// Field Constants
	public static final int Field_ARCFILEDATE = 1;
	public static final int Field_ARCFILEINDEX = 2;
	public static final int Field_ARCFILEOFFSET = 3;
	public static final int Field_COMPRESSEDSIZE = 4;
	public static final int Field_CRAWLNUMBER = 5;
	public static final int Field_DOCUMENTVERSION = 6;
	public static final int Field_PARSESEGMENTID = 7;
	public static final int Field_SIGNATURE = 8;
	public static final int Field_SIMHASH = 9;
	public static final int Field_CACHEITEMSRC = 10;
	static final int FieldID_MAX = Field_CACHEITEMSRC;

	// Field Declarations
	private BitSet __validFields = new BitSet(FieldID_MAX + 1);

	private long arcfileDate;
	private int arcfileIndex;
	private int arcfileOffset;
	private int compressedSize;
	private int crawlNumber;
	private int documentVersion;
	private int parseSegmentId;
	private FlexBuffer signature = new FlexBuffer();
	private long simhash;
	private TextBytes cacheItemSrc = new TextBytes();

	// Default Constructor
	public ArchiveInfo() {
	}

	// Accessors

	public final boolean isFieldDirty(int fieldId) {
		return __validFields.get(fieldId);
	}

	public final ArchiveInfo setFieldDirty(int fieldId) {
		__validFields.set(fieldId);
		return this;
	}

	public final ArchiveInfo setFieldClean(int fieldId) {
		__validFields.clear(fieldId);
		return this;
	}

	public long getArcfileDate() {
		return arcfileDate;
	}

	public ArchiveInfo setArcfileDate(long arcfileDate) {
		__validFields.set(Field_ARCFILEDATE);
		this.arcfileDate = arcfileDate;
		return this;
	}

	public int getArcfileIndex() {
		return arcfileIndex;
	}

	public ArchiveInfo setArcfileIndex(int arcfileIndex) {
		__validFields.set(Field_ARCFILEINDEX);
		this.arcfileIndex = arcfileIndex;
		return this;
	}

	public int getArcfileOffset() {
		return arcfileOffset;
	}

	public ArchiveInfo setArcfileOffset(int arcfileOffset) {
		__validFields.set(Field_ARCFILEOFFSET);
		this.arcfileOffset = arcfileOffset;
		return this;
	}

	public int getCompressedSize() {
		return compressedSize;
	}

	public ArchiveInfo setCompressedSize(int compressedSize) {
		__validFields.set(Field_COMPRESSEDSIZE);
		this.compressedSize = compressedSize;
		return this;
	}

	public int getCrawlNumber() {
		return crawlNumber;
	}

	public ArchiveInfo setCrawlNumber(int crawlNumber) {
		__validFields.set(Field_CRAWLNUMBER);
		this.crawlNumber = crawlNumber;
		return this;
	}

	public int getDocumentVersion() {
		return documentVersion;
	}

	public ArchiveInfo setDocumentVersion(int documentVersion) {
		__validFields.set(Field_DOCUMENTVERSION);
		this.documentVersion = documentVersion;
		return this;
	}

	public int getParseSegmentId() {
		return parseSegmentId;
	}

	public ArchiveInfo setParseSegmentId(int parseSegmentId) {
		__validFields.set(Field_PARSESEGMENTID);
		this.parseSegmentId = parseSegmentId;
		return this;
	}

	public ImmutableBuffer getSignature() {
		return new ImmutableBuffer(signature);
	}

	public ArchiveInfo setSignature(FlexBuffer signature) {
		__validFields.set(Field_SIGNATURE);
		this.signature = signature;
		return this;
	}

	public ArchiveInfo setSignature(Buffer signature, boolean shared) {
		__validFields.set(Field_SIGNATURE);
		this.signature = new FlexBuffer(signature.get(), 0, signature.getCount(), shared);
		return this;
	}

	public long getSimhash() {
		return simhash;
	}

	public ArchiveInfo setSimhash(long simhash) {
		__validFields.set(Field_SIMHASH);
		this.simhash = simhash;
		return this;
	}

	public TextBytes getCacheItemSrcAsTextBytes() {
		return cacheItemSrc;
	}

	public String getCacheItemSrc() {
		return cacheItemSrc.toString();
	}

	public ArchiveInfo setCacheItemSrc(String cacheItemSrc) {
		__validFields.set(Field_CACHEITEMSRC);
		this.cacheItemSrc.set(cacheItemSrc);
		return this;
	}

	// Object Dirty support 

	public final boolean isObjectDirty() {
		boolean isDirty = !__validFields.isEmpty();
		return isDirty;
	}

	// serialize implementation 
	public final void serialize(DataOutput output, BinaryProtocol encoder) throws java.io.IOException {
		encoder.beginFields(output);
		// serialize field:arcfileDate
		if (__validFields.get(Field_ARCFILEDATE)) {
			encoder.beginField(output, "arcfileDate", Field_ARCFILEDATE);
			encoder.writeVLong(output, arcfileDate);
		}
		// serialize field:arcfileIndex
		if (__validFields.get(Field_ARCFILEINDEX)) {
			encoder.beginField(output, "arcfileIndex", Field_ARCFILEINDEX);
			encoder.writeVInt(output, arcfileIndex);
		}
		// serialize field:arcfileOffset
		if (__validFields.get(Field_ARCFILEOFFSET)) {
			encoder.beginField(output, "arcfileOffset", Field_ARCFILEOFFSET);
			encoder.writeVInt(output, arcfileOffset);
		}
		// serialize field:compressedSize
		if (__validFields.get(Field_COMPRESSEDSIZE)) {
			encoder.beginField(output, "compressedSize", Field_COMPRESSEDSIZE);
			encoder.writeVInt(output, compressedSize);
		}
		// serialize field:crawlNumber
		if (__validFields.get(Field_CRAWLNUMBER)) {
			encoder.beginField(output, "crawlNumber", Field_CRAWLNUMBER);
			encoder.writeVInt(output, crawlNumber);
		}
		// serialize field:documentVersion
		if (__validFields.get(Field_DOCUMENTVERSION)) {
			encoder.beginField(output, "documentVersion", Field_DOCUMENTVERSION);
			encoder.writeVInt(output, documentVersion);
		}
		// serialize field:parseSegmentId
		if (__validFields.get(Field_PARSESEGMENTID)) {
			encoder.beginField(output, "parseSegmentId", Field_PARSESEGMENTID);
			encoder.writeVInt(output, parseSegmentId);
		}
		// serialize field:signature
		if (__validFields.get(Field_SIGNATURE)) {
			encoder.beginField(output, "signature", Field_SIGNATURE);
			encoder.writeFlexBuffer(output, signature);
		}
		// serialize field:simhash
		if (__validFields.get(Field_SIMHASH)) {
			encoder.beginField(output, "simhash", Field_SIMHASH);
			encoder.writeVLong(output, simhash);
		}
		// serialize field:cacheItemSrc
		if (__validFields.get(Field_CACHEITEMSRC)) {
			encoder.beginField(output, "cacheItemSrc", Field_CACHEITEMSRC);
			encoder.writeTextBytes(output, cacheItemSrc);
		}
		encoder.endFields(output);
	}

	// deserialize implementation 
	public final void deserialize(DataInput input, BinaryProtocol decoder) throws java.io.IOException {
		// clear existing data first  
		clear();

		// reset protocol object to unknown field id enconding mode (for compatibility)
		decoder.pushFieldIdEncodingMode(BinaryProtocol.FIELD_ID_ENCODING_MODE_UNKNOWN);
		// keep reading fields until terminator (-1) is located 
		int fieldId;
		while ((fieldId = decoder.readFieldId(input)) != -1) {
			switch (fieldId) {
			case Field_ARCFILEDATE: {
				__validFields.set(Field_ARCFILEDATE);
				arcfileDate = decoder.readVLong(input);
			}
				break;
			case Field_ARCFILEINDEX: {
				__validFields.set(Field_ARCFILEINDEX);
				arcfileIndex = decoder.readVInt(input);
			}
				break;
			case Field_ARCFILEOFFSET: {
				__validFields.set(Field_ARCFILEOFFSET);
				arcfileOffset = decoder.readVInt(input);
			}
				break;
			case Field_COMPRESSEDSIZE: {
				__validFields.set(Field_COMPRESSEDSIZE);
				compressedSize = decoder.readVInt(input);
			}
				break;
			case Field_CRAWLNUMBER: {
				__validFields.set(Field_CRAWLNUMBER);
				crawlNumber = decoder.readVInt(input);
			}
				break;
			case Field_DOCUMENTVERSION: {
				__validFields.set(Field_DOCUMENTVERSION);
				documentVersion = decoder.readVInt(input);
			}
				break;
			case Field_PARSESEGMENTID: {
				__validFields.set(Field_PARSESEGMENTID);
				parseSegmentId = decoder.readVInt(input);
			}
				break;
			case Field_SIGNATURE: {
				__validFields.set(Field_SIGNATURE);
				decoder.readFlexBuffer(input, signature);
			}
				break;
			case Field_SIMHASH: {
				__validFields.set(Field_SIMHASH);
				simhash = decoder.readVLong(input);
			}
				break;
			case Field_CACHEITEMSRC: {
				__validFields.set(Field_CACHEITEMSRC);
				decoder.readTextBytes(input, cacheItemSrc);
			}
				break;
			}
		}
		// pop extra encoding mode off of stack 
		decoder.popFieldIdEncodingMode();
	}

	// clear implementation 
	public final void clear() {
		__validFields.clear();
		arcfileDate = 0;
		arcfileIndex = 0;
		arcfileOffset = 0;
		compressedSize = 0;
		crawlNumber = 0;
		documentVersion = 0;
		parseSegmentId = 0;
		signature.reset();
		simhash = 0;
		cacheItemSrc.clear();
	}

	// equals implementation 
	public final boolean equals(final Object peer_) {
		if (!(peer_ instanceof ArchiveInfo)) {
			return false;
		}
		if (peer_ == this) {
			return true;
		}
		ArchiveInfo peer = (ArchiveInfo) peer_;
		boolean ret = __validFields.equals(peer.__validFields);
		if (!ret)
			return ret;
		if (__validFields.get(Field_ARCFILEDATE)) {
			ret = (arcfileDate == peer.arcfileDate);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_ARCFILEINDEX)) {
			ret = (arcfileIndex == peer.arcfileIndex);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_ARCFILEOFFSET)) {
			ret = (arcfileOffset == peer.arcfileOffset);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_COMPRESSEDSIZE)) {
			ret = (compressedSize == peer.compressedSize);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_CRAWLNUMBER)) {
			ret = (crawlNumber == peer.crawlNumber);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_DOCUMENTVERSION)) {
			ret = (documentVersion == peer.documentVersion);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_PARSESEGMENTID)) {
			ret = (parseSegmentId == peer.parseSegmentId);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_SIGNATURE)) {
			ret = signature.equals(peer.signature);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_SIMHASH)) {
			ret = (simhash == peer.simhash);
			if (!ret)
				return ret;
		}
		if (__validFields.get(Field_CACHEITEMSRC)) {
			ret = cacheItemSrc.equals(peer.cacheItemSrc);
			if (!ret)
				return ret;
		}
		return ret;
	}

	// clone implementation 
	@SuppressWarnings("unchecked")
	public final Object clone() throws CloneNotSupportedException {
		ArchiveInfo other = new ArchiveInfo();
		other.__validFields.or(this.__validFields);
		if (__validFields.get(Field_ARCFILEDATE)) {
			other.arcfileDate = this.arcfileDate;
		}
		if (__validFields.get(Field_ARCFILEINDEX)) {
			other.arcfileIndex = this.arcfileIndex;
		}
		if (__validFields.get(Field_ARCFILEOFFSET)) {
			other.arcfileOffset = this.arcfileOffset;
		}
		if (__validFields.get(Field_COMPRESSEDSIZE)) {
			other.compressedSize = this.compressedSize;
		}
		if (__validFields.get(Field_CRAWLNUMBER)) {
			other.crawlNumber = this.crawlNumber;
		}
		if (__validFields.get(Field_DOCUMENTVERSION)) {
			other.documentVersion = this.documentVersion;
		}
		if (__validFields.get(Field_PARSESEGMENTID)) {
			other.parseSegmentId = this.parseSegmentId;
		}
		if (__validFields.get(Field_SIGNATURE)) {
			other.signature = (FlexBuffer) this.signature.clone();
		}
		if (__validFields.get(Field_SIMHASH)) {
			other.simhash = this.simhash;
		}
		if (__validFields.get(Field_CACHEITEMSRC)) {
			other.cacheItemSrc = (TextBytes) this.cacheItemSrc.clone();
		}
		return other;
	}

	// merge implementation 
	@SuppressWarnings("unchecked")
	public final void merge(ArchiveInfo peer) throws CloneNotSupportedException {
		__validFields.or(peer.__validFields);
		if (peer.__validFields.get(Field_ARCFILEDATE)) {
			this.arcfileDate = peer.arcfileDate;
		}
		if (peer.__validFields.get(Field_ARCFILEINDEX)) {
			this.arcfileIndex = peer.arcfileIndex;
		}
		if (peer.__validFields.get(Field_ARCFILEOFFSET)) {
			this.arcfileOffset = peer.arcfileOffset;
		}
		if (peer.__validFields.get(Field_COMPRESSEDSIZE)) {
			this.compressedSize = peer.compressedSize;
		}
		if (peer.__validFields.get(Field_CRAWLNUMBER)) {
			this.crawlNumber = peer.crawlNumber;
		}
		if (peer.__validFields.get(Field_DOCUMENTVERSION)) {
			this.documentVersion = peer.documentVersion;
		}
		if (peer.__validFields.get(Field_PARSESEGMENTID)) {
			this.parseSegmentId = peer.parseSegmentId;
		}
		if (peer.__validFields.get(Field_SIGNATURE)) {
			this.signature = (FlexBuffer) peer.signature.clone();
		}
		if (peer.__validFields.get(Field_SIMHASH)) {
			this.simhash = peer.simhash;
		}
		if (peer.__validFields.get(Field_CACHEITEMSRC)) {
			this.cacheItemSrc = (TextBytes) peer.cacheItemSrc.clone();
		}
	}

	// hashCode implementation 
	public final int hashCode() {
		int result = 1;
		result = MurmurHash.hashLong(arcfileDate, result);
		result = MurmurHash.hashInt((int) arcfileIndex, result);
		result = MurmurHash.hashInt((int) arcfileOffset, result);
		result = MurmurHash.hashInt((int) compressedSize, result);
		result = MurmurHash.hashInt((int) crawlNumber, result);
		result = MurmurHash.hashInt((int) documentVersion, result);
		result = MurmurHash.hashInt((int) parseSegmentId, result);
		result = MurmurHash.hash(signature.get(), signature.getOffset(), signature.getCount(), result);
		result = MurmurHash.hashLong(simhash, result);
		result = MurmurHash.hash(cacheItemSrc.getBytes(), cacheItemSrc.getOffset(), cacheItemSrc.getLength(), result);
		return result;
	}
}
